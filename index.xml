<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>neural niche</title>
    <link>http://neuralniche.com/</link>
    <description>Recent content on neural niche</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>ðŸ˜ˆ</copyright>
    <lastBuildDate>Wed, 17 Feb 2016 01:56:25 -0800</lastBuildDate>
    <atom:link href="http://neuralniche.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>TensorFlow 0.7.0 dockerfile with Python 3</title>
      <link>http://neuralniche.com/post/tensorflow/</link>
      <pubDate>Wed, 17 Feb 2016 01:56:25 -0800</pubDate>
      
      <guid>http://neuralniche.com/post/tensorflow/</guid>
      <description>

&lt;h1 id=&#34;tensorflow:1d21a3c703f1d61655064657ff3833ef&#34;&gt;TensorFlow&lt;/h1&gt;

&lt;p&gt;In 2015 Google came out with a new deep learning framework/tensor library similar in many ways to Theano and I enjoy using it a lot more than Theano simply due to long compile times of Theano when using Keras and TensorBoard.  This will not go into detail about using Theano or TensorFlow or Keras but instead is how I built a docker image that uses a slightly older nvidia card (which for my purposes is capable of using multiple gpu&amp;rsquo;s in isolation and exiting a model on one card and not effecting the other).&lt;/p&gt;

&lt;h2 id=&#34;background:1d21a3c703f1d61655064657ff3833ef&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;The most recent version of TensorFlow 0.7.0 just came out, and while i had built a docker image for python3 with tensorflow 0.6.0, it was using cuda v7.0 and cudnn2.  While there is nothing vastly different about these, their is definitely an advantage to using a more recent cuDNN for CNN&amp;rsquo;s from my testing with Keras.  Along with this, there is far less altering of permissions with a dockerfile that can cause frustrating issues.  I had tried to see if anyone had written up anything similar and ran into this post (but this post uses 7.0 and cuDNN v2):&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://eatcodeplay.com/installing-gpu-enabled-tensorflow-with-python-3-4-in-ec2/&#34;&gt;installing-gpu-enabled-tensorflow-with-python-3-4-in-ec2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I had previously built a docker image prior to this post that I believe was faster than the stated ~75 minutes due to using a prebuilt image from nvidia (and not having to go through the process to obtain cudnn access) but had not really posted most about it since I was unsure how useful it was and there were python2 dockerbuilds of tensorflow available with GPU support.  While this isn&amp;rsquo;t vastly different than using a prebuilt ami or something similar it allows quicker spinup time than reinstalling for multiple VM&amp;rsquo;s and more maintainability in a system with multiple gpu&amp;rsquo;s with Keras by allowing specific gpu&amp;rsquo;s to be used via a docker system (as well as using multiple different cuda/cudnn versions on the same system with far less hassle).  Along with this, the more I use Docker, the more I come to enjoy using it for data science/deep learning tasks due to the ability to control and isolate models and the ability to create data pipelines with ease.
Majority of my dockerfile is based off the original &lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu&#34;&gt;google dockerfile&lt;/a&gt; but original configuration doesn&amp;rsquo;t work with older nvidia cards or python3.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/r/grahama/tf/&#34;&gt;Docker Hub Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;python3.Dockerfile&#34;&gt;TensorFlow 0.6.0 Dockerfile&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;but wanted to fix it for 0.7.0 and found the blog post and figured I would write this up.&lt;/p&gt;

&lt;h2 id=&#34;requirements:1d21a3c703f1d61655064657ff3833ef&#34;&gt;Requirements&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;GPU with 3.0&amp;gt; cuda compute capabilities&lt;/li&gt;
&lt;li&gt;Docker &amp;gt;= 1.9&lt;/li&gt;
&lt;li&gt;Nvidia drivers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: Since I am using the Nvidia Grid K2, this may not be the most efficient or best way to build docker images for gpu&amp;rsquo;s with cuda capabilities &amp;gt;3.0&lt;/p&gt;

&lt;h3 id=&#34;nvidia-drivers:1d21a3c703f1d61655064657ff3833ef&#34;&gt;Nvidia Drivers:&lt;/h3&gt;

&lt;p&gt;For Docker and Nvidia drivers I have a setup.sh file that uses 352 since 361 seems to cause issues.  A simplified form of what I install (via a setup.sh script) is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-get update &amp;amp;&amp;amp; apt-get install -y sshfs curl wget git htop vim software-properties-common
add-apt-repository ppa:graphics-drivers/ppa -y
apt-get update &amp;amp;&amp;amp; apt-get install -y nvidia-352 nvidia-settings


# Docker and Docker-Compose Stuff
curl -sSL https://get.docker.com/ | sh
curl -L https://github.com/docker/compose/releases/download/1.5.2/docker-compose-`uname -s`-`uname -m` &amp;gt; /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose

git clone https://github.com/NVIDIA/nvidia-docker
cd nvidia-docker &amp;amp;&amp;amp; make install
nvidia-docker volume setup
nvidia-docker run nvidia/cuda nvidia-smi
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;new-dockerfile:1d21a3c703f1d61655064657ff3833ef&#34;&gt;New Dockerfile&lt;/h2&gt;

&lt;p&gt;The new dockerfile isnt terribly different from my original (and would translate easy to just a normal VM on amazon or what not) but requires a few changes from my original file that I am listing for my own clarity.  The full dockerfile is here: &lt;a href=&#34;tensorflow070.Dockerfile&#34;&gt;Dockerfile&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, changing to get-pip.py install of pip3 vs apt-get install python3-pip due to version number and error with protobuf with the severely outdated official 14.04 repo version.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Second, using the latest version of Bazel seems to work which seems smart to track along with due to it being a google product that I am somewhat assuming the tensorflow team uses a recent version of.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Third, setting env so config builds.  Straight forward using the official nvidia dockerfile and locating required files including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CUDA_TOOLKIT_PATH=/usr/local/cuda-7.5&lt;/li&gt;
&lt;li&gt;CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu&lt;/li&gt;
&lt;li&gt;TF_NEED_CUDA=1&lt;/li&gt;
&lt;li&gt;PYTHON_BIN_PATH=/usr/bin/python3&lt;/li&gt;
&lt;li&gt;TF_CUDA_COMPUTE_CAPABILITIES=&amp;ldquo;3.0&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lastly, symlink the cudnn.h file since it is not in the default CUDNN_INSTALL_PATH:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ln -s /usr/include/cudnn.h /usr/lib/x86_64-linux-gnu/cudnn.h&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these env variables are findable using find | grep (and this works for python 3.5 as well).&lt;/p&gt;

&lt;p&gt;The dockerfile also includes a bunch of extra stuff to install Keras and have it default to TensorFlow (which I then use with a custom script based off the official docker_run_gpu.sh from TensorFlow to customize which GPU is being used by which model being run and allows CPU restriction as well).  While there is other examples of people using TensorFlow with GPU through Docker, all the previous examples were using Python2.7&lt;/p&gt;

&lt;h2 id=&#34;any-questions-or-need-help:1d21a3c703f1d61655064657ff3833ef&#34;&gt;Any questions or need help?&lt;/h2&gt;

&lt;p&gt;I would love to help with any aspects regarding Docker/Keras/TensorFlow
&lt;a href=&#34;mailto:graham.annett@gmail.com&#34;&gt;Email Me&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>using generative neural nets in keras to create â€˜on-the-flyâ€™ dialogue</title>
      <link>http://neuralniche.com/post/tutorial/</link>
      <pubDate>Thu, 10 Sep 2015 12:02:41 -0700</pubDate>
      
      <guid>http://neuralniche.com/post/tutorial/</guid>
      <description>

&lt;h3 id=&#34;introduction:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;introduction:&lt;/h3&gt;

&lt;p&gt;there&amp;rsquo;s been a few cool things done with generative neural nets so far but to my knowledge, very few generative neural nets have found a useful application in any business realm (or has been posted about).  this is by no means the best use or the most interesting but, i think it is an incredibly interesting idea and is a potential starting point for generative neural nets to be utilized in a way that is incredibly beneficial for training or as a useful tool.&lt;/p&gt;

&lt;p&gt;there&amp;rsquo;s  a lot of potential for this and other similar sorts of technologies and i&amp;rsquo;d love to work on or collaborate with others on something.  if you are interested please contact me at the email listed on the bottom of this post.&lt;/p&gt;

&lt;h2 id=&#34;let-s-start:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;let&amp;rsquo;s start&lt;/h2&gt;

&lt;p&gt;my initial idea started with the idea that the tons and tons of dialogue that youtube videos have already (closed captioning) which is already a massive dataset and could potentially be used to train a lot of different machine learning models.  i think it also has the ability to resemble human dialogue somewhat (and shows promise over a lot of similar attempts at NLP concepts i have experimented with).  it may not be &lt;em&gt;perfect&lt;/em&gt;, but it does seem to carry many of the interesting inflections and peculiarities of human speech that written word does not always capture.&lt;/p&gt;

&lt;p&gt;the training set to create this model is just a collection of youtube vids that deal with sales or call oriented dialogue, for instance heres a couple of the videos i used:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=3fbmf2IAEVM&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/3fbmf2IAEVM/0.jpg&#34; alt=&#34;example 1&#34; /&gt;&lt;/a&gt;
&lt;a href=&#34;http://www.youtube.com/watch?v=4ostqJD3Psc&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/4ostqJD3Psc/0.jpg&#34; alt=&#34;example 1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;there&amp;rsquo;s not a particular reason i used any of these videos other than they are very long, may have phone dialogue, and have subtitles/closed captions already.  i tried to find vids that seemed like the captions were somewhat accurate but this was very haphazard and i&amp;rsquo;m sure it&amp;rsquo;s effect on the training set is noticeable.  i won&amp;rsquo;t claim to have watched these vids or to even know what they are discussing, but that&amp;rsquo;s kind of why i think this technology is interesting.  also the cc are not perfect and it&amp;rsquo;s easy to see they have a lot of mistakes, if you have a suggestion for how to fix this, please let me know!&lt;/p&gt;

&lt;p&gt;from here, all that is necessary now is to collect enough videos for a string length of greater than ~500k.&lt;/p&gt;

&lt;p&gt;i have a python script that uses youtube-dl and pysrt to grab the subtitles/closed captions.  you don&amp;rsquo;t need pysrt since the subtitles are very standardized but it&amp;rsquo;s useful if you wish to venture to stuff outside of youtube.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import youtube_dl, pysrt
import numpy as np

class audio_source(object):
    def __init__(self, url):
        self.url = url
        self.ydl_opts = {
            &#39;subtitles&#39;: &#39;en&#39;,
            &#39;writesubtitles&#39;: True,
            &#39;writeautomaticsub&#39;: True}

        self.subtitlesavailable = self.are_subs_available()

        if self.subtitlesavailable:
            self.grab_auto_subs()

    def are_subs_available(self):
        with youtube_dl.YoutubeDL(self.ydl_opts) as ydl:
            subs = ydl.extract_info(self.url, download=False)
        if subs[&#39;requested_subtitles&#39;]:
            self.title = subs[&#39;title&#39;]
            self.subs_url = subs[&#39;requested_subtitles&#39;][&#39;en&#39;][&#39;url&#39;]
            return True
        else:
            return False

    def grab_auto_subs(self):
        &amp;quot;&amp;quot;&amp;quot;
        grab&#39;s subs or cc depending on whats available,
        think it grabs both if subtitles are available
        issue with ydl_opts but doesn&#39;t bother me
        &amp;quot;&amp;quot;&amp;quot;
        try:
            urllib.request.urlretrieve(
                self.subs_url, &#39;youtube-dl-texts/&#39; + self.title + &#39;.srt&#39;)
            print(&amp;quot;subtitles saved directly from youtube\n&amp;quot;)
            text = pysrt.open(&#39;youtube-dl-texts/&#39; + self.title + &#39;.srt&#39;)
            self.text = text.text.replace(&#39;\n&#39;, &#39; &#39;)
        except IOError:
            print(&amp;quot;\n *** saving sub&#39;s didn&#39;t work *** \n&amp;quot;)

with open(&#39;other/url_list&#39;,&#39;r&#39;) as datafile:
    url_list = datafile.read().splitlines()

total_text = []

for u in url_list:
    try:
        total_text.append(audio_source(url=u).text)
    except AttributeError:
        pass
total_text = &#39; &#39;.join(total_text).lower()

print(len(total_text))

&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;training-the-generative-neural-net:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;training the generative neural net&lt;/h2&gt;

&lt;p&gt;at this point you have a mass of text that if you read it, probably looks pretty incoherent and useless (also i&amp;rsquo;m not creating a separation between texts like many other text&amp;rsquo;s have and would probably be very useful in disseminating when a conversation should be ended etc).  the great thing is that &lt;em&gt;hopefully&lt;/em&gt; there is enough data to create an end result for the time being and the errors will regress to the mean so to speak (with long enough text&amp;rsquo;s there seems to be truth to this).&lt;/p&gt;

&lt;p&gt;here&amp;rsquo;s an example of some of the last 260 chars of the dialogue i have from slightly less than 1 MB worth of text from videos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print(total_text[-260:])
&amp;gt;&amp;gt;&amp;gt;&#39;more information about those meetings and travel make sure to fax it to this number at the bottom and are you into the grand prize drawing weeks stay at intercontinental resort Tahiti be sure to fax in that form you all right thank you feel you have a great day&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to train the model we first need to do a bit of preprocessing since the generative neural net uses sequential data character by character (well in &lt;em&gt;steps&lt;/em&gt;, but character by character for each step (a fair amount of this is from the &lt;a href=&#34;https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py&#34;&gt;keras LSTM generating example&lt;/a&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chars = set(total_text)

char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))


maxlen = 20
step = 1
sentences = []
next_chars = []
for i in range(0, len(total_text) - maxlen, step):
    sentences.append(total_text[i: i + maxlen])
    next_chars.append(total_text[i + maxlen])
print(&#39;nb sequences:&#39;, len(sentences))



X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        X[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;lstm-training:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;LSTM training&lt;/h4&gt;

&lt;p&gt;for a NN library, i am using keras for a few reasons but it is so far my favorite python NN library due to how modular and easy to understand it is (and the &lt;a href=&#34;https://github.com/fchollet/&#34;&gt;creator&lt;/a&gt; and contributors seem incredibly smart)
using keras and a LSTM architecture is the easiest to show but there&amp;rsquo;s tons of ways to improve this as well (different layers, cascading architectures, etc). play around and let me know if you find something superior!&lt;/p&gt;

&lt;p&gt;one of the better models i found:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM


model = Sequential()
# input -&amp;gt; layer 1
model.add(LSTM(len(chars), 512, return_sequences=True))
model.add(Dropout(0.20))
# use 20% dropout on all LSTM layers: http://arxiv.org/abs/1312.4569

# 1.5 testing
model.add(LSTM(512, 512, return_sequences=True))
model.add(Dropout(0.20))
# layer 2
model.add(LSTM(512, 256, return_sequences=True))
model.add(Dropout(0.20))
# layer 3
model.add(LSTM(256, 256, return_sequences=False))
model.add(Dropout(0.20))
# layer 4 -&amp;gt; output
model.add(Dense(256, len(chars)))
model.add(Activation(&#39;softmax&#39;))

# compile or load weights then compile depending
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;)

model.fit(X,y,nb_epoch=50)

&amp;gt;&amp;gt;&amp;gt; Epoch 0
&amp;gt;&amp;gt;&amp;gt; 7744/285648 [&amp;gt;.............................] - ETA: 4717s - loss: 3.0232
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;if you would like to see a rudimentary visualization of the architecture:
&lt;img src=&#34;http://neuralniche.com/images/123.png&#34; alt=&#34;nn arch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;this will probably take quite awhile using a personal computer even with a GPU.  From my experimentation it is optimal to get a loss to around .8 - .4 which takes around 40-50 epoch&amp;rsquo;s&lt;/p&gt;

&lt;p&gt;at this point, a model is trained and we are ready to generate some recommended dialogue&lt;/p&gt;

&lt;h2 id=&#34;generate-some-text:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;generate some text&lt;/h2&gt;

&lt;p&gt;the final part of this is being able to speak something to your computer (or potentially, computer would listen to what you or someone else is saying in some app or extension) and from there get the speech into text form to generate a suggestion of what to follow that sentence with.&lt;/p&gt;

&lt;p&gt;there&amp;rsquo;s a few ways to do this but the easiest is to register and get an API key for google speech to text, and install some libraries to be able to use the &lt;a href=&#34;https://pypi.python.org/pypi/SpeechRecognition/&#34;&gt;python speech recognition module&lt;/a&gt;
use a personal key in the Recognizer since otherwise it won&amp;rsquo;t work for other&amp;rsquo;s using the module once it hit&amp;rsquo;s 50 queries.  you need to subscribe to a mailing list and then &lt;a href=&#34;http://www.chromium.org/developers/how-tos/api-keys&#34;&gt;enable the api but it takes about 2 minutes.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;you can incorporate whatever was spoken into the model as well, but that&amp;rsquo;s for a later date.  right now, all i will do is set it up so you speak to it for a moment and then it generates some text and prints that out.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import speech_recognition as sr

recognizer = sr.Recognizer(key=myKey)

def speech2text(r=recognizer):

    # speak to microphone, use google api, return text
    input(&#39;press enter then speak: \n&#39;+&#39;------&#39;*5)
    with sr.Microphone() as source:
        audio = r.listen(source)
        try:
            print(&#39;\nprocessing...\n&#39;)
            return r.recognize(audio).lower()
        except LookupError:
            pass

def gentext():

    seed_text = speech2text()
    generated = &#39;&#39; + seed_text
    print(&#39;------&#39;*5+&#39;\nyou said: \n&#39;+&#39;&amp;quot;&#39; + seed_text +&#39;&amp;quot;&#39;)


    print(&#39;------&#39;*5+&#39;\n generating...\n&#39;+ &#39;------&#39;*5)
    for iteration in range(50):
        # create x vector from seed to predict off of
        x = np.zeros((1, len(seed_text), len(chars)))
        for t, char in enumerate(seed_text):
            x[0, t, char_indices[char]] = 1.

        preds = model.predict(x, verbose=0)[0]
        next_index = np.argmax(preds)
        next_char = indices_char[next_index]

        generated += next_char
        seed_text = seed_text[1:] + next_char
    print(&#39;\n\nfollow up with: &#39; + generated)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;here&amp;rsquo;s one of the best single example&amp;rsquo;s i&amp;rsquo;ve found with this model and have gotten a lot different results from a variety of attempts and architecture&amp;rsquo;s and planning on posting more if it would be useful (or can post the exact architecture and model weights which keras can then load):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gentext()

press enter then speak:
------------------------------

processing...

------------------------------
you said:
&amp;quot;i would like to talk to you about a house i saw that you had for sale&amp;quot;
------------------------------
 generating...
------------------------------

follow up with:
i would like to talk to you about a house i saw that you had for sale tell me what was its price though and i can reall
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;note:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;note:&lt;/h4&gt;

&lt;p&gt;in terms of training the model, training/predicting with a GPU vs CPU is about 3-4x faster on my 2013 macbook pro&lt;/p&gt;

&lt;h2 id=&#34;conclusion:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;conclusion&lt;/h2&gt;

&lt;p&gt;with something like this, it&amp;rsquo;s very easy to see how you could splice in audio from a phone call or text chat that this would carry over very well to.  given the right data set&amp;rsquo;s theres tons of potential uses.  along with this, there&amp;rsquo;s also ways to stack and blend models together that provide different and separate different dialogue/differentiate people within dialogue.&lt;/p&gt;

&lt;p&gt;this all came about because after the yc fellowship rejection, i thought i may as well post this since i personally think it has a ton of potential.  if you are interested in hearing more about this, or hearing more about this type of stuff, contact me at the email posted below or signup for a newsletter.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;a href=&#34;mailto:graham.annett@gmail.com&#34;&gt;contact me&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;link href=&#34;//cdn-images.mailchimp.com/embedcode/slim-081711.css&#34; rel=&#34;stylesheet&#34; type=&#34;text/css&#34;&gt;
&lt;style type=&#34;text/css&#34;&gt;
    #mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
&lt;/style&gt;
&lt;div id=&#34;mc_embed_signup&#34;&gt;
&lt;form action=&#34;//neuralniche.us11.list-manage.com/subscribe/post?u=7f3e6432894032f97ce9da591&amp;amp;id=a86a7392be&#34; method=&#34;post&#34; id=&#34;mc-embedded-subscribe-form&#34; name=&#34;mc-embedded-subscribe-form&#34; class=&#34;validate&#34; target=&#34;_blank&#34; novalidate&gt;
&lt;div id=&#34;mc_embed_signup_scroll&#34;&gt;
&lt;label for=&#34;mce-EMAIL&#34;&gt;Subscribe to our mailing list&lt;/label&gt;
&lt;input type=&#34;email&#34; value=&#34;&#34; name=&#34;EMAIL&#34; class=&#34;email&#34; id=&#34;mce-EMAIL&#34; placeholder=&#34;email address&#34; required&gt;
&lt;div style=&#34;position: absolute; left: -5000px;&#34;&gt;&lt;input type=&#34;text&#34; name=&#34;b_7f3e6432894032f97ce9da591_a86a7392be&#34; tabindex=&#34;-1&#34; value=&#34;&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;clear&#34;&gt;&lt;input type=&#34;submit&#34; value=&#34;Subscribe&#34; name=&#34;subscribe&#34; id=&#34;mc-embedded-subscribe&#34; class=&#34;button&#34;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/form&gt;
&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>hey!</title>
      <link>http://neuralniche.com/post/first/</link>
      <pubDate>Wed, 19 Aug 2015 12:02:41 -0700</pubDate>
      
      <guid>http://neuralniche.com/post/first/</guid>
      <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Will update&lt;/p&gt;

&lt;div class=&#34;sendgrid-subscription-widget&#34; data-token=&#34;s%2FyRM37csnDZE2YNQgMgA%2FrRJMnvJB6PyYYUOnu3QGxoLuVzko05dg04VrpUhqXh&#34;
    &lt;form&gt;
        &lt;div class=&#34;response&#34;&gt;&lt;/div&gt;
        &lt;label&gt;
        &lt;br /&gt;
            &lt;span&gt;interested in using this, enter email below!&lt;/span&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;input type=&#34;email&#34; name=&#34;email&#34; placeholder=&#34;enter email&#34; /&gt;
        &lt;/label&gt;
        &lt;input type=&#34;submit&#34; value=&#34;submit :)&#34; /&gt;
    &lt;/form&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>about neural niche</title>
      <link>http://neuralniche.com/about/</link>
      <pubDate>Wed, 19 Aug 2015 12:02:02 -0700</pubDate>
      
      <guid>http://neuralniche.com/about/</guid>
      <description>

&lt;h2 id=&#34;about-neural-niche:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;about neural niche:&lt;/h2&gt;

&lt;p&gt;ideas, plans, and guides to use generative neural networks and machine learning in way&amp;rsquo;s that are both novel and useful&lt;/p&gt;

&lt;h2 id=&#34;contact-me:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;contact me:&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;mailto:graham.annett@gmail.com&#34;&gt;graham&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>