<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>neural niche</title>
    <link>http://neuralniche.com/</link>
    <description>Recent content on neural niche</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>ðŸ˜ˆ</copyright>
    <lastBuildDate>Wed, 19 Aug 2015 14:39:51 -0700</lastBuildDate>
    <atom:link href="http://neuralniche.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>using generative neural nets to create â€˜on-the-flyâ€™ phone dialogue</title>
      <link>http://neuralniche.com/post/tutorial/</link>
      <pubDate>Wed, 19 Aug 2015 14:39:51 -0700</pubDate>
      
      <guid>http://neuralniche.com/post/tutorial/</guid>
      <description>

&lt;h3 id=&#34;introduction:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;introduction:&lt;/h3&gt;

&lt;p&gt;there&amp;rsquo;s been a few cool things done with generative neural nets so far but to my knowledge, very few generative neural nets have found a useful application in any industry.  while i doubt this will be the best use, i think it is an incredibly interesting idea.&lt;/p&gt;

&lt;p&gt;there&amp;rsquo;s  a lot of potential for this and other similar sorts of technologies and i&amp;rsquo;d love to work on or collaborate with others on something.  if you are interested please contact me at the email listed on the bottom of this post.  Thanks!&lt;/p&gt;

&lt;h2 id=&#34;let-s-start:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;let&amp;rsquo;s start&lt;/h2&gt;

&lt;p&gt;my initial idea started with the idea that the tons and tons of dialogue that youtube videos have already (closed captioning) is already a massive dataset and could potentially be used to train a lot of different machine learning models.  i think it also has the ability to resemble human dialogue somewhat and is in an easily mineable place (well for its volume, i couldn&amp;rsquo;t find anything comparable).  it may not be &lt;em&gt;perfect&lt;/em&gt;, but it does seem to carry many of the interesting inflections and peculuarities of human speech that writen word does not always capture.&lt;/p&gt;

&lt;p&gt;the initial training set is just a collection of youtube vids that deal with sales or call oriented dialogue, for instance heres a couple of the videos i used:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=3fbmf2IAEVM&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/3fbmf2IAEVM/0.jpg&#34; alt=&#34;example 1&#34; /&gt;&lt;/a&gt;
&lt;a href=&#34;http://www.youtube.com/watch?v=4ostqJD3Psc&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/4ostqJD3Psc/0.jpg&#34; alt=&#34;example 1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;there&amp;rsquo;s not a particular reason i used any of these videos other than they are very long, have phone dialogue, and have subtitles/closed captions already.  i won&amp;rsquo;t claim to have watched these vids or to even know what they are discussing, but that&amp;rsquo;s kind of why i think this technology is interesting.  also the cc are not perfect and it&amp;rsquo;s easy to see they have a lot of mistakes, if you have a suggestion for how to fix this, please let me know!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
from here, all that is necesarry now is to collect enough videos for a string length of ~500k.  this is the most tedious part as dialogue in srt or txt form usually is ~10 kb&amp;hellip; to get a sizeable amount of text you will need to spend a bit of time searching around (or just collect everything related, random links etc but then you will start noticeably getting poor examples)&lt;/p&gt;

&lt;p&gt;from here, i have a python script that uses youtube-dl and pysrt to grab the subtitles/closed captions.  you don&amp;rsquo;t need pysrt since the subtitles are very standardized but it&amp;rsquo;s useful if you wish to venture to stuff outside of youtube.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import youtube_dl, pysrt
import numpy as np

class audio_source(object):
    def __init__(self, url):
        self.url = url
        self.ydl_opts = {
            &#39;subtitles&#39;: &#39;en&#39;,
            &#39;writesubtitles&#39;: True,
            &#39;writeautomaticsub&#39;: True}

        self.subtitlesavailable = self.are_subs_available()

        if self.subtitlesavailable:
            self.grab_auto_subs()

    def are_subs_available(self):
        with youtube_dl.YoutubeDL(self.ydl_opts) as ydl:
            subs = ydl.extract_info(self.url, download=False)
        if subs[&#39;requested_subtitles&#39;]:
            self.title = subs[&#39;title&#39;]
            self.subs_url = subs[&#39;requested_subtitles&#39;][&#39;en&#39;][&#39;url&#39;]
            return True
        else:
            return False

    def grab_auto_subs(self):
        &amp;quot;&amp;quot;&amp;quot;
        grab&#39;s subs or cc depending on whats available,
        think it grabs both if subtitles are available
        issue with ydl_opts but doesn&#39;t bother me
        &amp;quot;&amp;quot;&amp;quot;
        try:
            urllib.request.urlretrieve(
                self.subs_url, &#39;youtube-dl-texts/&#39; + self.title + &#39;.srt&#39;)
            print(&amp;quot;subtitles saved directly from youtube\n&amp;quot;)
            text = pysrt.open(&#39;youtube-dl-texts/&#39; + self.title + &#39;.srt&#39;)
            self.text = text.text.replace(&#39;\n&#39;, &#39; &#39;)
        except IOError:
            print(&amp;quot;\n *** saving sub&#39;s didn&#39;t work *** \n&amp;quot;)


total_text = []

for u in url_list:
    try:
        total_text.append(audio_source(url=u).text)
    except AttributeError:
        pass
total_text = &#39; &#39;.join(total_text).lower()

print(len(total_text))

&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;training-the-generative-neural-net:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;training the generative neural net&lt;/h2&gt;

&lt;p&gt;at this point you have a mass of text that if you read it, probably looks pretty incoherent and useless.  the great thing is that &lt;em&gt;hopefully&lt;/em&gt; there is enough data to create an &amp;lsquo;okay&amp;rsquo; end result for the time being and the errors will regress to the mean so to speak.&lt;/p&gt;

&lt;p&gt;here&amp;rsquo;s an example of some of the last 260 chars of the dialogue i have from about 1 MB worth of text from videos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print(total_text[-260:])
&amp;gt;&amp;gt;&amp;gt;&#39;bit more information about those meetings and travel sure to fax it to this number at the bottom and are you into the grand prize drawing weeks stay at intercontinental resort Tahiti he sure to fax in that form you all right thank you feel you have a great day&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to train the model we first need to do a bit of preprocessing since the generative nn uses sequential data character by character (unlike say markov chains, which while they can, become nonsensical incredibly quick if using char)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chars = set(total_text)

char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))


maxlen = 25
step = 5
sentences = []
next_chars = []
for i in range(0, len(total_text) - maxlen, step):
    sentences.append(total_text[i: i + maxlen])
    next_chars.append(total_text[i + maxlen])
print(&#39;nb sequences:&#39;, len(sentences))



X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        X[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;lstm-training:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;LSTM training&lt;/h4&gt;

&lt;p&gt;using keras and a LSTM architecture is the easiest to show but there&amp;rsquo;s tons of ways to improve this as well (different layers, architectures, etc). play around and let me know if you find something vastly superior!&lt;/p&gt;

&lt;p&gt;initially just build the model:&lt;/p&gt;

&lt;p&gt;TODO below:
 2. compare softmax on activation function&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM


model = Sequential()

# input -&amp;gt; layer 1
model.add(LSTM(len(chars), 512, return_sequences=True))
model.add(Dropout(0.25))
# use dropout on all LSTM layers: http://arxiv.org/abs/1312.4569

# layer 2
model.add(LSTM(512, 512, return_sequences=True))
model.add(Dropout(0.2))

# layer 3
model.add(LSTM(512, 512, return_sequences=False))
model.add(Dropout(0.2))

# layer 4 -&amp;gt; output
model.add(Dense(512, len(chars)))
model.add(Activation(&#39;softplus&#39;))


# need to compile model to fit it
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;just a heads up: to get a worthwhile model you will need to have a sizeable training set, with this sizeable training set it will result in the model taking quite awhile to fit.  anything less than ~50 epochs is kind of useless also.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model.fit(X,y,nb_epoch=100)

&amp;gt;&amp;gt;&amp;gt; Epoch 0
&amp;gt;&amp;gt;&amp;gt; 24960/146862 [====&amp;gt;.........................] - ETA: 7838s - loss: 3.0381
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;at this point, a model is trained and we are ready to generate some dialogue and scripts&lt;/p&gt;

&lt;h2 id=&#34;generate-some-text:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;generate some text&lt;/h2&gt;

&lt;p&gt;the final part of this is being able to speak something to your computer (or theoretically, computer would listen to what you or someone else is saying in some app or extension) and from there get the speech into text form and generate a sequential prediction.&lt;/p&gt;

&lt;p&gt;there&amp;rsquo;s a few ways to do this but the easiest is to register and get an API key for google speech to text, and install some libraries to be able to use the python module:
(&lt;a href=&#34;https://pypi.python.org/pypi/SpeechRecognition/&#34;&gt;https://pypi.python.org/pypi/SpeechRecognition/&lt;/a&gt;)
use a personal key in the Recognizer since otherwise it won&amp;rsquo;t work for other&amp;rsquo;s using the module once it hit&amp;rsquo;s 50 queries (i believe).  you need to subscribe to a mailing list and then enable the api but it takes about 2 minutes.
(&lt;a href=&#34;http://www.chromium.org/developers/how-tos/api-keys&#34;&gt;http://www.chromium.org/developers/how-tos/api-keys&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;you can incorporate the text into the model as well, but that&amp;rsquo;s for a later date.  right now, all i will do is set it up so you speak to it for a moment and then it generates some text and prints that out.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import speech_recognition as sr

recognizer = sr.Recognizer(key=keyIPtied)

def speech2text(r=recognizer):
    # speak to microphone, use google api, return text
    input(&amp;quot;press enter then speak&amp;quot;)
    with sr.Microphone() as source:
        audio = r.listen(source)
        try:
            return r.recognize(audio).lower()
        except LookupError:
            pass

def gentext():

    seed_text = speech2text()
    generated = &#39;&#39; + seed_text

    print(&#39;\n&#39;*5 + &#39;------&#39;*5)
    print(&#39;you said: &#39; + seed_text)
    print(&#39;------&#39;*5)
    for iteration in range(50):
        # create x vector from seed to predict off of
        x = np.zeros((1, len(seed_text), len(chars)))
        for t, char in enumerate(seed_text):
            x[0, t, char_indices[char]] = 1.

        preds = m2.predict(x, verbose=0)[0]
        next_index = np.argmax(preds)
        next_char = indices_char[next_index]

        generated += next_char
        seed_text = seed_text[1:] + next_char
    print(&#39;follow up with: &#39; + generated)

gentext()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;let&amp;rsquo;s try it out, here&amp;rsquo;s an example gif of what you could theoreticaly expect:
&lt;img src=&#34;../../../images/testing.gif&#34; alt=&#34;testing gif&#34; /&gt;&lt;/p&gt;

&lt;p&gt;here&amp;rsquo;s some great examples i&amp;rsquo;ve found with this model and have gotten a lot different results from a variety of attempts:&lt;/p&gt;

&lt;h4 id=&#34;note:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;note:&lt;/h4&gt;

&lt;p&gt;something also fun to notice is if you set the nb_epochs a bit too low, you can get incredibly interesting&lt;/p&gt;

&lt;h2 id=&#34;conclusion:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;conclusion&lt;/h2&gt;

&lt;p&gt;with something like this, it&amp;rsquo;s very easy to see how you could splice in audio from a phone call or text chat that this would carry over very well to.  given the right data set&amp;rsquo;s theres tons of potential uses.  along with this, there&amp;rsquo;s also ways to stack and blend models together that provide different and useful results as well.&lt;/p&gt;

&lt;p&gt;this all came about because after the yc fellowship rejection, i thought i may as well post this since i personally think it has a ton of potential.  if you are interested in hearing more about this, or hearing more about this type of stuff, contact me at the email posted below or signup for a newsletter.  thanks!
&lt;br /&gt;
&lt;a href=&#34;mailto:graham.annett@gmail.com&#34;&gt;contact me&lt;/a&gt;&lt;/p&gt;

&lt;!-- Begin MailChimp Signup Form --&gt;

&lt;p&gt;&lt;link href=&#34;//cdn-images.mailchimp.com/embedcode/slim-081711.css&#34; rel=&#34;stylesheet&#34; type=&#34;text/css&#34;&gt;
&lt;style type=&#34;text/css&#34;&gt;
    #mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
&lt;/style&gt;
&lt;div id=&#34;mc_embed_signup&#34;&gt;
&lt;form action=&#34;//neuralniche.us11.list-manage.com/subscribe/post?u=7f3e6432894032f97ce9da591&amp;amp;id=a86a7392be&#34; method=&#34;post&#34; id=&#34;mc-embedded-subscribe-form&#34; name=&#34;mc-embedded-subscribe-form&#34; class=&#34;validate&#34; target=&#34;_blank&#34; novalidate&gt;
&lt;div id=&#34;mc_embed_signup_scroll&#34;&gt;
&lt;label for=&#34;mce-EMAIL&#34;&gt;Subscribe to our mailing list&lt;/label&gt;
&lt;input type=&#34;email&#34; value=&#34;&#34; name=&#34;EMAIL&#34; class=&#34;email&#34; id=&#34;mce-EMAIL&#34; placeholder=&#34;email address&#34; required&gt;
&lt;div style=&#34;position: absolute; left: -5000px;&#34;&gt;&lt;input type=&#34;text&#34; name=&#34;b_7f3e6432894032f97ce9da591_a86a7392be&#34; tabindex=&#34;-1&#34; value=&#34;&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;clear&#34;&gt;&lt;input type=&#34;submit&#34; value=&#34;Subscribe&#34; name=&#34;subscribe&#34; id=&#34;mc-embedded-subscribe&#34; class=&#34;button&#34;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/form&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;!--End mc_embed_signup--&gt;
</description>
    </item>
    
    <item>
      <title>hey there</title>
      <link>http://neuralniche.com/post/first/</link>
      <pubDate>Wed, 19 Aug 2015 12:02:41 -0700</pubDate>
      
      <guid>http://neuralniche.com/post/first/</guid>
      <description>

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&#34;tutorial-coming-soon:e8fb9c67eba912c72729806db31eaa1b&#34;&gt;tutorial coming soon&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
will have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;example&lt;/li&gt;
&lt;li&gt;example code&lt;/li&gt;
&lt;li&gt;ipython notebook&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&#34;sendgrid-subscription-widget&#34; data-token=&#34;s%2FyRM37csnDZE2YNQgMgA%2FrRJMnvJB6PyYYUOnu3QGxoLuVzko05dg04VrpUhqXh&#34;
    &lt;form&gt;
        &lt;div class=&#34;response&#34;&gt;&lt;/div&gt;
        &lt;label&gt;
        &lt;br /&gt;
            &lt;span&gt;interested in using this, enter email below!&lt;/span&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;input type=&#34;email&#34; name=&#34;email&#34; placeholder=&#34;enter email&#34; /&gt;
        &lt;/label&gt;
        &lt;input type=&#34;submit&#34; value=&#34;submit :)&#34; /&gt;
    &lt;/form&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>about neural niche</title>
      <link>http://neuralniche.com/about/</link>
      <pubDate>Wed, 19 Aug 2015 12:02:02 -0700</pubDate>
      
      <guid>http://neuralniche.com/about/</guid>
      <description>

&lt;h2 id=&#34;about-neural-niche:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;about neural niche:&lt;/h2&gt;

&lt;p&gt;ideas, plans, and guides to use generative neural networks and machine learning in way&amp;rsquo;s that are both novel and useful&lt;/p&gt;

&lt;h2 id=&#34;contact-me:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;contact me:&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;mailto:graham.annett@gmail.com&#34;&gt;graham&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://neuralniche.com/post/tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://neuralniche.com/post/tutorial/</guid>
      <description>&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&#34;utf-8&#34;&gt;&lt;style&gt;body {
  width: 45em;
  border: 1px solid #ddd;
  outline: 1300px solid #fff;
  margin: 16px auto;
}

body .markdown-body
{
  padding: 30px;
}

@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format(&#39;woff&#39;);
}

.markdown-body {
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333;
  overflow: hidden;
  font-family: &#34;Helvetica Neue&#34;, Helvetica, &#34;Segoe UI&#34;, Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body strong {
  font-weight: bold;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type=&#34;checkbox&#34;] {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px/1.4 Helvetica, arial, freesans, clean, sans-serif, &#34;Segoe UI Emoji&#34;, &#34;Segoe UI Symbol&#34;;
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before {
  display: table;
  content: &#34;&#34;;
}

.markdown-body hr:after {
  display: table;
  clear: both;
  content: &#34;&#34;;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code {
  font: 12px Consolas, &#34;Liberation Mono&#34;, Menlo, Courier, monospace;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
  font: 12px Consolas, &#34;Liberation Mono&#34;, Menlo, Courier, monospace;
}

.markdown-body .octicon {
  font: normal normal 16px octicons-anchor;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.markdown-body .octicon-link:before {
  content: &#39;\f05c&#39;;
}

.markdown-body&gt;*:first-child {
  margin-top: 0 !important;
}

.markdown-body&gt;*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .anchor {
  position: absolute;
  top: 0;
  bottom: 0;
  left: 0;
  display: block;
  padding-right: 6px;
  padding-left: 30px;
  margin-left: -30px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  position: relative;
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  display: none;
  color: #000;
  vertical-align: middle;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  padding-left: 8px;
  margin-left: -30px;
  line-height: 1;
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  display: inline-block;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li&gt;p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote&gt;:first-child {
  margin-top: 0;
}

.markdown-body blockquote&gt;:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: &#34;\00a0&#34;;
}

.markdown-body pre&gt;code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

.markdown-body .highlight {
  background: #fff;
}

.markdown-body .highlight .h {
  color: #333;
  font-style: normal;
  font-weight: normal;
}

.markdown-body .highlight .mf,
.markdown-body .highlight .mh,
.markdown-body .highlight .mi,
.markdown-body .highlight .mo,
.markdown-body .highlight .il,
.markdown-body .highlight .m {
  color: #945277;
}

.markdown-body .highlight .s,
.markdown-body .highlight .sb,
.markdown-body .highlight .sc,
.markdown-body .highlight .sd,
.markdown-body .highlight .s2,
.markdown-body .highlight .se,
.markdown-body .highlight .sh,
.markdown-body .highlight .si,
.markdown-body .highlight .sx,
.markdown-body .highlight .s1 {
  color: #df5000;
}

.markdown-body .highlight .kc,
.markdown-body .highlight .kd,
.markdown-body .highlight .kn,
.markdown-body .highlight .kp,
.markdown-body .highlight .kr,
.markdown-body .highlight .kt,
.markdown-body .highlight .k,
.markdown-body .highlight .o {
  font-weight: bold;
}

.markdown-body .highlight .kt {
  color: #458;
}

.markdown-body .highlight .c,
.markdown-body .highlight .cm,
.markdown-body .highlight .c1 {
  color: #998;
  font-style: italic;
}

.markdown-body .highlight .cp,
.markdown-body .highlight .cs,
.markdown-body .highlight .cp .h {
  color: #999;
  font-weight: bold;
}

.markdown-body .highlight .cs {
  font-style: italic;
}

.markdown-body .highlight .n {
  color: #333;
}

.markdown-body .highlight .na,
.markdown-body .highlight .nv,
.markdown-body .highlight .vc,
.markdown-body .highlight .vg,
.markdown-body .highlight .vi {
  color: #008080;
}

.markdown-body .highlight .nb {
  color: #0086B3;
}

.markdown-body .highlight .nc {
  color: #458;
  font-weight: bold;
}

.markdown-body .highlight .no {
  color: #094e99;
}

.markdown-body .highlight .ni {
  color: #800080;
}

.markdown-body .highlight .ne {
  color: #990000;
  font-weight: bold;
}

.markdown-body .highlight .nf {
  color: #945277;
  font-weight: bold;
}

.markdown-body .highlight .nn {
  color: #555;
}

.markdown-body .highlight .nt {
  color: #000080;
}

.markdown-body .highlight .err {
  color: #a61717;
  background-color: #e3d2d2;
}

.markdown-body .highlight .gd {
  color: #000;
  background-color: #fdd;
}

.markdown-body .highlight .gd .x {
  color: #000;
  background-color: #faa;
}

.markdown-body .highlight .ge {
  font-style: italic;
}

.markdown-body .highlight .gr {
  color: #aa0000;
}

.markdown-body .highlight .gh {
  color: #999;
}

.markdown-body .highlight .gi {
  color: #000;
  background-color: #dfd;
}

.markdown-body .highlight .gi .x {
  color: #000;
  background-color: #afa;
}

.markdown-body .highlight .go {
  color: #888;
}

.markdown-body .highlight .gp {
  color: #555;
}

.markdown-body .highlight .gs {
  font-weight: bold;
}

.markdown-body .highlight .gu {
  color: #800080;
  font-weight: bold;
}

.markdown-body .highlight .gt {
  color: #aa0000;
}

.markdown-body .highlight .ow {
  font-weight: bold;
}

.markdown-body .highlight .w {
  color: #bbb;
}

.markdown-body .highlight .sr {
  color: #017936;
}

.markdown-body .highlight .ss {
  color: #8b467f;
}

.markdown-body .highlight .bp {
  color: #999;
}

.markdown-body .highlight .gc {
  color: #999;
  background-color: #EAF2F5;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  display: inline-block;
  padding: 3px 5px;
  font: 11px Consolas, &#34;Liberation Mono&#34;, Menlo, Courier, monospace;
  line-height: 10px;
  color: #000;
  border: 1px solid #cfcfcf;
  border-radius: 2px;
}

.markdown-body .highlight .pl-coc,
.markdown-body .highlight .pl-entm,
.markdown-body .highlight .pl-eoa,
.markdown-body .highlight .pl-mai .pl-sf,
.markdown-body .highlight .pl-pdv,
.markdown-body .highlight .pl-sc,
.markdown-body .highlight .pl-sr,
.markdown-body .highlight .pl-v,
.markdown-body .highlight .pl-vpf {
  color: #0086b3;
}

.markdown-body .highlight .pl-eoac,
.markdown-body .highlight .pl-mdht,
.markdown-body .highlight .pl-mi1,
.markdown-body .highlight .pl-mri,
.markdown-body .highlight .pl-va,
.markdown-body .highlight .pl-vpu {
  color: #008080;
}

.markdown-body .highlight .pl-c,
.markdown-body .highlight .pl-pdc {
  color: #b4b7b4;
  font-style: italic;
}

.markdown-body .highlight .pl-k,
.markdown-body .highlight .pl-ko,
.markdown-body .highlight .pl-kolp,
.markdown-body .highlight .pl-mc,
.markdown-body .highlight .pl-mr,
.markdown-body .highlight .pl-ms,
.markdown-body .highlight .pl-s,
.markdown-body .highlight .pl-sok,
.markdown-body .highlight .pl-st {
  color: #6e5494;
}

.markdown-body .highlight .pl-ef,
.markdown-body .highlight .pl-enf,
.markdown-body .highlight .pl-enm,
.markdown-body .highlight .pl-entc,
.markdown-body .highlight .pl-eoi,
.markdown-body .highlight .pl-sf,
.markdown-body .highlight .pl-smc {
  color: #d12089;
}

.markdown-body .highlight .pl-ens,
.markdown-body .highlight .pl-eoai,
.markdown-body .highlight .pl-kos,
.markdown-body .highlight .pl-mh .pl-pdh,
.markdown-body .highlight .pl-mp,
.markdown-body .highlight .pl-pde,
.markdown-body .highlight .pl-stp {
  color: #458;
}

.markdown-body .highlight .pl-enti {
  color: #d12089;
  font-weight: bold;
}

.markdown-body .highlight .pl-cce,
.markdown-body .highlight .pl-enc,
.markdown-body .highlight .pl-kou,
.markdown-body .highlight .pl-mq {
  color: #f93;
}

.markdown-body .highlight .pl-mp1 .pl-sf {
  color: #458;
  font-weight: bold;
}

.markdown-body .highlight .pl-cos,
.markdown-body .highlight .pl-ent,
.markdown-body .highlight .pl-md,
.markdown-body .highlight .pl-mdhf,
.markdown-body .highlight .pl-ml,
.markdown-body .highlight .pl-pdc1,
.markdown-body .highlight .pl-pds,
.markdown-body .highlight .pl-s1,
.markdown-body .highlight .pl-scp,
.markdown-body .highlight .pl-sol {
  color: #df5000;
}

.markdown-body .highlight .pl-c1,
.markdown-body .highlight .pl-cn,
.markdown-body .highlight .pl-pse,
.markdown-body .highlight .pl-pse .pl-s2,
.markdown-body .highlight .pl-vi {
  color: #a31515;
}

.markdown-body .highlight .pl-mb,
.markdown-body .highlight .pl-pdb {
  color: #df5000;
  font-weight: bold;
}

.markdown-body .highlight .pl-mi,
.markdown-body .highlight .pl-pdi {
  color: #6e5494;
  font-style: italic;
}

.markdown-body .highlight .pl-ms1 {
  background-color: #f5f5f5;
}

.markdown-body .highlight .pl-mdh,
.markdown-body .highlight .pl-mdi {
  font-weight: bold;
}

.markdown-body .highlight .pl-mdr {
  color: #0086b3;
  font-weight: bold;
}

.markdown-body .highlight .pl-s2 {
  color: #333;
}

.markdown-body .highlight .pl-ii {
  background-color: #df5000;
  color: #fff;
}

.markdown-body .highlight .pl-ib {
  background-color: #f93;
}

.markdown-body .highlight .pl-id {
  background-color: #a31515;
  color: #fff;
}

.markdown-body .highlight .pl-iu {
  background-color: #b4b7b4;
}

.markdown-body .highlight .pl-mo {
  color: #969896;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  float: left;
  margin: 0.3em 0 0.25em -1.6em;
  vertical-align: middle;
}&lt;/style&gt;&lt;title&gt;tutorial&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;article class=&#34;markdown-body&#34;&gt;&lt;p&gt;+++
date = &#34;2015-08-19T14:39:51-07:00&#34;
draft = false
title = &#34;using generative neural nets to create â€˜on-the-flyâ€™ phone dialogue&#34;
author = &#34;&#34;
+++&lt;/p&gt;

&lt;h3&gt;
&lt;a id=&#34;user-content-introduction&#34; class=&#34;anchor&#34; href=&#34;#introduction&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;introduction:&lt;/h3&gt;

&lt;p&gt;initially i had an idea for a startup for the yc fellowship, but given that it was rejected, i thought it would be fun to post a write up of what i&#39;ve done (well not everything...)&lt;/p&gt;

&lt;p&gt;there&#39;s  a lot of potential for this and other similar sorts of technologies and i&#39;d love to work on or collaborate with others on something.  if you are interested please contact me at the email listed on the bottom of this post.  Thanks!&lt;/p&gt;

&lt;h2&gt;
&lt;a id=&#34;user-content-lets-start&#34; class=&#34;anchor&#34; href=&#34;#lets-start&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;let&#39;s start&lt;/h2&gt;

&lt;p&gt;my initial idea started with the idea that the tons and tons of dialogue that youtube videos have already (closed captioning) is already a massive dataset and could potentially be used to train a lot of different machine learning models.  i think it also has the ability to resemble human dialogue somewhat and is in an easily mineable place (well for its volume, i couldn&#39;t find anything comparable).  it may not be &lt;em&gt;perfect&lt;/em&gt;, but it does seem to carry many of the interesting inflections and peculuarities of human speech that writen word does not always capture.&lt;/p&gt;

&lt;p&gt;the initial training set is just a collection of youtube vids that deal with sales or call oriented dialogue, for instance heres a couple of the videos i used:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=3fbmf2IAEVM&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/b41bbadd1b19979032c4e7ec2ff89b8ebfbf3b6e/687474703a2f2f696d672e796f75747562652e636f6d2f76692f3366626d6632494145564d2f302e6a7067&#34; alt=&#34;example 1&#34; data-canonical-src=&#34;http://img.youtube.com/vi/3fbmf2IAEVM/0.jpg&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;http://www.youtube.com/watch?v=4ostqJD3Psc&#34;&gt;&lt;img src=&#34;https://camo.githubusercontent.com/d2f1401b8b0be9b7001106a17db5a9f0ce8f5da7/687474703a2f2f696d672e796f75747562652e636f6d2f76692f346f7374714a44335073632f302e6a7067&#34; alt=&#34;example 1&#34; data-canonical-src=&#34;http://img.youtube.com/vi/4ostqJD3Psc/0.jpg&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;there&#39;s not a particular reason i used any of these videos other than they are very long, have phone dialogue, and have subtitles/closed captions already.  i won&#39;t claim to have watched these vids or to even know what they are discussing, but that&#39;s kind of why i think this technology is interesting.  also the cc are not perfect and it&#39;s easy to see they have a lot of mistakes, if you have a suggestion for how to fix this, please let me know!&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
from here, all that is necesarry now is to collect enough videos for a string length of ~500k.  this is the most tedious part as dialogue in srt or txt form usually is ~10 kb... to get a sizeable amount of text you will need to spend a bit of time searching around (or just collect everything related, random links etc but then you will start noticeably getting poor examples)&lt;/p&gt;

&lt;p&gt;from here, i have a python script that uses youtube-dl and pysrt to grab the subtitles/closed captions.  you don&#39;t need pysrt since the subtitles are very standardized but it&#39;s useful if you wish to venture to stuff outside of youtube.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import youtube_dl, pysrt
class audio_source(object):
    def __init__(self, url):
        self.url = url
        self.ydl_opts = {
            &#39;subtitles&#39;: &#39;en&#39;,
            &#39;writesubtitles&#39;: True,
            &#39;writeautomaticsub&#39;: True}

        self.subtitlesavailable = self.are_subs_available()

        if self.subtitlesavailable:
            self.grab_auto_subs()

    def are_subs_available(self):
        with youtube_dl.YoutubeDL(self.ydl_opts) as ydl:
            subs = ydl.extract_info(self.url, download=False)
        if subs[&#39;requested_subtitles&#39;]:
            self.title = subs[&#39;title&#39;]
            self.subs_url = subs[&#39;requested_subtitles&#39;][&#39;en&#39;][&#39;url&#39;]
            return True
        else:
            return False

    def grab_auto_subs(self):
        &#34;&#34;&#34;
        grab&#39;s subs or cc depending on whats available,
        think it grabs both if subtitles are available
        issue with ydl_opts but doesn&#39;t bother me
        &#34;&#34;&#34;
        try:
            urllib.request.urlretrieve(
                self.subs_url, &#39;youtube-dl-texts/&#39; + self.title + &#39;.srt&#39;)
            print(&#34;subtitles saved directly from youtube\n&#34;)
            text = pysrt.open(&#39;youtube-dl-texts/&#39; + self.title + &#39;.srt&#39;)
            self.text = text.text.replace(&#39;\n&#39;, &#39; &#39;)
        except IOError:
            print(&#34;\n *** saving sub&#39;s didn&#39;t work *** \n&#34;)


total_text = []

for u in url_list:
    try:
        total_text.append(audio_source(url=u).text)
    except AttributeError:
        pass
total_text = &#39; &#39;.join(total_text).lower()

print(len(total_text))

&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;
&lt;a id=&#34;user-content-training-the-generative-neural-net&#34; class=&#34;anchor&#34; href=&#34;#training-the-generative-neural-net&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;training the generative neural net&lt;/h2&gt;

&lt;p&gt;at this point you have a mass of text that if you read it, probably looks pretty incoherent and useless.  the great thing is that &lt;em&gt;hopefully&lt;/em&gt; there is enough data to create an &#39;okay&#39; end result for the time being and the errors will regress to the mean so to speak.&lt;/p&gt;

&lt;p&gt;here&#39;s an example of some of the last 260 chars of the dialogue i have from about 1 MB worth of text from videos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print(total_text[-260:])
&amp;gt;&amp;gt;&amp;gt;&#39;bit more information about those meetings and travel sure to fax it to this number at the bottom and are you into the grand prize drawing weeks stay at intercontinental resort Tahiti he sure to fax in that form you all right thank you feel you have a great day&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to train the model we first need to do a bit of preprocessing since the generative nn uses sequential data character by character (unlike say markov chains, which while they can, become nonsensical incredibly quick if using char)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chars = set(total_text)

char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))


maxlen = 50
step = 3
sentences = []
next_chars = []
for i in range(0, len(total_text) - maxlen, step):
    sentences.append(total_text[i: i + maxlen])
    next_chars.append(total_text[i + maxlen])
print(&#39;nb sequences:&#39;, len(sentences))



X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        X[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4&gt;
&lt;a id=&#34;user-content-lstm-training&#34; class=&#34;anchor&#34; href=&#34;#lstm-training&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;LSTM training&lt;/h4&gt;

&lt;p&gt;using keras and a LSTM architecture is the easiest to show but there&#39;s tons of ways to improve this as well (different layers, architectures, etc). play around and let me know if you find something vastly superior!&lt;/p&gt;

&lt;p&gt;initially just build the model:&lt;/p&gt;

&lt;p&gt;TODO below:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;compare with 128, 512 as well&lt;/li&gt;
&lt;li&gt;compare softmax on activation function&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM
from keras.datasets.data_utils import get_file



model = Sequential()

# input -&amp;gt; layer 1
model.add(LSTM(len(chars), 512, return_sequences=True))
model.add(Dropout(0.25))
# use dropout on all LSTM layers: http://arxiv.org/abs/1312.4569

# layer 2
model.add(LSTM(512, 512, return_sequences=True))
model.add(Dropout(0.2))

# layer 3
model.add(LSTM(512, 512, return_sequences=False))
model.add(Dropout(0.2))

# layer 4 -&amp;gt; output
model.add(Dense(512, len(chars)))
model.add(Activation(&#39;softplus&#39;))


# need to compile model to fit it
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;just a heads up: to get a worthwhile model you will need to have a sizeable training set, with this sizeable training set it will result in the model taking quite awhile to fit.  expect a few hours if you dont adjust any parameters.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model.fit(X,y)

&amp;gt;&amp;gt;&amp;gt; Epoch 0
&amp;gt;&amp;gt;&amp;gt; 24960/146862 [====&amp;gt;.........................] - ETA: 7838s - loss: 3.0381
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;at this point, a model is trained and we are ready to generate some dialogue and scripts&lt;/p&gt;

&lt;h2&gt;
&lt;a id=&#34;user-content-generate-some-text&#34; class=&#34;anchor&#34; href=&#34;#generate-some-text&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;generate some text&lt;/h2&gt;

&lt;p&gt;the final part of this is being able to speak something to your computer (or theoretically, computer would listen to what you or someone else is saying in some app or extension) and from there get the speech into text form and generate a sequential prediction.&lt;/p&gt;

&lt;p&gt;there&#39;s a few ways to do this but the easiest is to register and get an API key for google speech to text, and install some libraries to be able to use the python module:
(&lt;a href=&#34;https://pypi.python.org/pypi/SpeechRecognition/&#34;&gt;https://pypi.python.org/pypi/SpeechRecognition/&lt;/a&gt;)
use a personal key in the Recognizer since otherwise it won&#39;t work for other&#39;s using the module once it hit&#39;s 50 queries (i believe).  you need to subscribe to a mailing list and then enable the api but it takes about 2 minutes.
(&lt;a href=&#34;http://www.chromium.org/developers/how-tos/api-keys&#34;&gt;http://www.chromium.org/developers/how-tos/api-keys&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;you can incorporate the text into the model as well, but that&#39;s for a later date.  right now, all i will do is set it up so you speak to it for a moment and then it generates some text and prints that out.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import speech_recognition as sr

# key is ip tied so you must use your own
keyIPtied=&#39;AIzaSyASM587FctrnOddazt6LK7z80797NTckmk&#39;

recognizer = sr.Recognizer(key=keyIPtied)

def s2t():
    input(&#34;press enter then speak&#34;)
    with sr.Microphone() as source:
        audio = r.listen(source)
        try:
            return r.recognize(audio).lower()
        except LookupError:
            pass

def pick_index(a, temperature=1.0):
    a = np.log(a) / temperature
    a = np.exp(a) / np.sum(np.exp(a))
    return np.argmax(np.random.multinomial(1, a, 1))

seed_text = s2t()

generated = &#39;&#39; + seed_text

# create x vector to predict off of
x = np.zeros((1, maxlen, len(chars)))
for t, char in enumerate(seed_text):
    x[0, t, char_indices[char]] = 1.

preds = model.predict(x, verbose=0)[0]
next_index = pick_index(preds, diversity)
next_char = indices_char[next_index]

generated += next_char
sentence = sentence[1:] + next_char

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;let&#39;s try it out, here&#39;s an example gif of what you could theoreticaly expect:
&lt;a href=&#34;../../../images/testing.gif&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;../../../images/testing.gif&#34; alt=&#34;testing gif&#34; style=&#34;max-width:100%;&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;here&#39;s some great examples i&#39;ve found with this model and have gotten a lot different results from a variety of attempts:&lt;/p&gt;

&lt;h2&gt;
&lt;a id=&#34;user-content-conclusion&#34; class=&#34;anchor&#34; href=&#34;#conclusion&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;octicon octicon-link&#34;&gt;&lt;/span&gt;&lt;/a&gt;conclusion&lt;/h2&gt;

&lt;p&gt;with something like this, it&#39;s very easy to see how you could splice in audio from a phone call or text chat that this would carry over very well to.  given the right data set&#39;s theres tons of potential uses.  along with this, there&#39;s also ways to stack and blend models together that provide different and useful results as well.&lt;/p&gt;

&lt;p&gt;if you are interested in hearing more about this, or hearing more about this type of stuff, contact me at the email posted below or signup for a newsletter.  thanks!
&lt;br&gt;
&lt;a href=&#34;mailto:graham.annett@gmail.com&#34;&gt;contact me&lt;/a&gt;&lt;/p&gt;



&lt;p&gt;&lt;/p&gt;


    #mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */


&lt;div id=&#34;user-content-mc_embed_signup&#34;&gt;

&lt;div id=&#34;user-content-mc_embed_signup_scroll&#34;&gt;
Subscribe to our mailing list

&lt;div&gt;&lt;/div&gt;
&lt;div&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;/article&gt;&lt;/body&gt;&lt;/html&gt;</description>
    </item>
    
  </channel>
</rss>