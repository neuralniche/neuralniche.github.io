<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on neural niche</title>
    <link>http://neuralniche.com/post/</link>
    <description>Recent content in Posts on neural niche</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>ðŸ˜ˆ</copyright>
    <lastBuildDate>Fri, 13 May 2016 15:59:47 -0700</lastBuildDate>
    <atom:link href="http://neuralniche.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>telegram bot</title>
      <link>http://neuralniche.com/post/telegram-bot/</link>
      <pubDate>Fri, 13 May 2016 15:59:47 -0700</pubDate>
      
      <guid>http://neuralniche.com/post/telegram-bot/</guid>
      <description>

&lt;h1 id=&#34;setting-up-a-telegram-bot-in-python-and-docker:56c792581b660f87618a1a730906e1b5&#34;&gt;Setting up a Telegram Bot in Python and Docker&lt;/h1&gt;

&lt;p&gt;This post is my personal introduction to using Telegram and Bluemix and while it is incredibly simple, it is useful to see how to do the basics before integrating peripheral API&amp;rsquo;s or extraneous processes.&lt;/p&gt;

&lt;p&gt;First, set up a telegram account however you&amp;rsquo;d like.  I personally used the &lt;a href=&#34;https://desktop.telegram.org&#34;&gt;Desktop client&lt;/a&gt; and it worked great.  Once you have an account, you need to interact with @BotFather to set up a bot.  Message @BotFather /start and you will receive these options:
&lt;img src=&#34;http://neuralniche.com/images/bot-father-start.png&#34; width=&#34;200&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Reply with /newbot and walk through the steps to get a bot with a name and handle.  Also /setinline to allow inline of bot.&lt;/p&gt;

&lt;h3 id=&#34;purpose:56c792581b660f87618a1a730906e1b5&#34;&gt;Purpose&lt;/h3&gt;

&lt;p&gt;For this I just wanted to experiment with telegram and see how to do very simple basic interactions and deploy it via Bluemix.&lt;/p&gt;

&lt;h2 id=&#34;set-up-docker:56c792581b660f87618a1a730906e1b5&#34;&gt;Set up Docker&lt;/h2&gt;

&lt;p&gt;I want to allow the entire bot to be run as a Docker container to allow for easy deployment to any cloud container service as well as make changes and push an image without having to worry about much else.  To do this, if you already are using Docker it just requires adding the cf CLI tool and a plugin to interface with the hosted images/containers.
This is fairly straightforward and the official directions &lt;a href=&#34;https://console.ng.bluemix.net/docs/containers/container_cli_ov.html&#34;&gt;from here&lt;/a&gt; are incredibly concise.  Make sure you can login (or if using a different platform, have the ability to push images) and from there you can easily push images.&lt;/p&gt;

&lt;h2 id=&#34;using-python-to-interact-with-inline-queries-and-commands:56c792581b660f87618a1a730906e1b5&#34;&gt;Using Python to interact with Inline Queries and Commands&lt;/h2&gt;

&lt;p&gt;There is an easy to use and well maintained python module for the telegram API.  I&amp;rsquo;m using python rather than something like node to interface with keras/tensorflow/etc in the future for various tasks and want to simplify and streamline my learning process. &lt;a href=&#34;https://github.com/python-telegram-bot/python-telegram-bot&#34;&gt;python-telegram-bot&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Setting up the bot we are just going to be doing a single inline ability and a single telegram command.  It&amp;rsquo;s fairly straightforward and I used most of the &lt;a href=&#34;https://github.com/python-telegram-bot/python-telegram-bot/blob/master/examples/inlinebot.py&#34;&gt;example file&lt;/a&gt; from the python-telegram-bot library for simplicity but it consists of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from configparser import ConfigParser
import random

from uuid import uuid4 #used to create individual id&#39;s for querys
from telegram import InlineQueryResultArticle, InputTextMessageContent
from telegram.ext import Updater, InlineQueryHandler, CommandHandler

vids = [...list of assorted videos...]

def get_config_token():
    config = ConfigParser()
    config.read_file(open(&#39;config&#39;))
    token = config[&#39;default&#39;][&#39;token&#39;]
    return token

def tellem(bot, update):
    bot.sendMessage(update.message.chat_id, text=msg + random.choice(vids))

def inlinequery(bot, update):
    results = list()
    results.append(InlineQueryResultArticle(
        id=uuid4(),
        title=&amp;quot;tell em&amp;quot;,
        input_message_content=InputTextMessageContent(msg +
                                                      random.choice(vids))))
    bot.answerInlineQuery(update.inline_query.id, results=results)

def main():
    # updater with config
    updater = Updater(get_config_token())

    # Get the dispatcher to register handlers
    d = updater.dispatcher

    # example of standard command
    d.addHandler(CommandHandler(&amp;quot;tellem&amp;quot;, tellem))

    # inline part
    d.addHandler(InlineQueryHandler(inlinequery))

    # Start the Bot
    updater.start_polling()

    # Block until the user presses Ctrl-C or the process receives SIGINT,
    updater.idle()

if __name__ == &#39;__main__&#39;:
    main()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From here, we will deploy with a Docker image to Bluemix.  Since the bot is incredibly simple, the Dockerfile isn&amp;rsquo;t much:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM python:3.5-onbuild

ENTRYPOINT [&amp;quot;python&amp;quot;, &amp;quot;main.py&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since the python:#-onbuild image takes a requirements.txt and installs dependencies for us, make a requirements.txt file and put &amp;ldquo;python-telegram-bot&amp;rdquo; in it since that is the only dependency.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m also reading the API token from a config file that looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[default]
token = &#39;token here&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then to build and deploy you need to have the Bluemix command line tool and the &lt;a href=&#34;https://console.ng.bluemix.net/docs/containers/container_cli_ov.html#container_cli_ov&#34;&gt;containers command line plugin&lt;/a&gt; that we (hopefully) previously installed and run the following commands for an image/bot named Testbot on a your personal registry called your-registry:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Docker build -t Testbot .
Docker tag Testbot registry.ng.bluemix.net/your-registry/Testbot:latest
Docker push registry.ng.bluemix.net/your-registry/Testbot:latest
cf ic run --name testbot registry.ng.bluemix.net/your-registry/Testbot:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last command, which uses cf ic is how you will interact with the containers and for instance to stop this bot would require you to run &amp;ldquo;cf ic stop -t 1 testbot&amp;rdquo;.  The bot is now running and available to anyone.  An example of how it will look via inline and using standard command:
&lt;img src=&#34;http://neuralniche.com/images/souljabotexample.png&#34; width=&#34;250&#34;/&gt;&lt;/p&gt;

&lt;p&gt;With this basic outline you can easily add other functionality and using @BotFather you can also add an image, help text, etc.  This could also easily be done on a multitude of other platforms besides IBM&amp;rsquo;s Bluemix.&lt;/p&gt;

&lt;p&gt;Using Bluemix I will next use a Watson API to do something actually useful (will post soon).&lt;/p&gt;

&lt;p&gt;If you are looking to make a bot to do something or integrate with the IBM Bluemix Platform let me know, I&amp;rsquo;d love to help!
&lt;a href=&#34;mailto:graham.annett@gmail.com&#34;&gt;Email Me&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TensorFlow 0.7.0 dockerfile with Python 3</title>
      <link>http://neuralniche.com/post/tensorflow/</link>
      <pubDate>Wed, 17 Feb 2016 01:56:25 -0800</pubDate>
      
      <guid>http://neuralniche.com/post/tensorflow/</guid>
      <description>

&lt;p&gt;edit: everything has since been updated to &lt;a href=&#34;tensorflow070.Dockerfile&#34;&gt;Tensorflow 0.7.0&lt;/a&gt; which I based off of my &lt;a href=&#34;base.Dockerfile&#34;&gt;base cuda dockerfile&lt;/a&gt; to use with tensorflow or theano (depending on my goals, Keras allows great flexibility in between training vs compiling)&lt;/p&gt;

&lt;h1 id=&#34;tensorflow:1d21a3c703f1d61655064657ff3833ef&#34;&gt;TensorFlow&lt;/h1&gt;

&lt;p&gt;In 2015 Google came out with a new deep learning framework/tensor library similar in many ways to Theano and I enjoy using it a lot more than Theano simply due to long compile times of Theano when using Keras and TensorBoard.  This will not go into detail about using Theano or TensorFlow or Keras but instead is how I built a docker image that uses a slightly older nvidia card (which for my purposes is capable of using multiple gpu&amp;rsquo;s in isolation and exiting a model on one card and not effecting the other).&lt;/p&gt;

&lt;h2 id=&#34;background:1d21a3c703f1d61655064657ff3833ef&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;The most recent version of TensorFlow 0.7.0 just came out, and while i had built a docker image for python3 with tensorflow 0.6.0, it was using cuda v7.0 and cudnn2.  While there is nothing vastly different about these, their is definitely an advantage to using a more recent cuDNN for CNN&amp;rsquo;s from my testing with Keras.  Along with this, there is far less altering of permissions with a dockerfile that can cause frustrating issues.  I had tried to see if anyone had written up anything similar and ran into this post (but this post uses 7.0 and cuDNN v2):&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://eatcodeplay.com/installing-gpu-enabled-tensorflow-with-python-3-4-in-ec2/&#34;&gt;installing-gpu-enabled-tensorflow-with-python-3-4-in-ec2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I had previously built a docker image prior to this post that I believe was faster than the stated ~75 minutes due to using a prebuilt image from nvidia (and not having to go through the process to obtain cudnn access) but had not really posted most about it since I was unsure how useful it was and there were python2 dockerbuilds of tensorflow available with GPU support.  While this isn&amp;rsquo;t vastly different than using a prebuilt ami or something similar it allows quicker spinup time than reinstalling for multiple VM&amp;rsquo;s and more maintainability in a system with multiple gpu&amp;rsquo;s with Keras by allowing specific gpu&amp;rsquo;s to be used via a docker system (as well as using multiple different cuda/cudnn versions on the same system with far less hassle).  Along with this, the more I use Docker, the more I come to enjoy using it for data science/deep learning tasks due to the ability to control and isolate models and the ability to create data pipelines with ease.
Majority of my dockerfile is based off the original &lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu&#34;&gt;google dockerfile&lt;/a&gt; but original configuration doesn&amp;rsquo;t work with older nvidia cards or python3.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/r/grahama/tf/&#34;&gt;Docker Hub Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;python3.Dockerfile&#34;&gt;TensorFlow 0.6.0 Dockerfile&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;but wanted to fix it for 0.7.0 and found the blog post and figured I would write this up.&lt;/p&gt;

&lt;h2 id=&#34;requirements:1d21a3c703f1d61655064657ff3833ef&#34;&gt;Requirements&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;GPU with 3.0&amp;gt; cuda compute capabilities&lt;/li&gt;
&lt;li&gt;Docker &amp;gt;= 1.9&lt;/li&gt;
&lt;li&gt;Nvidia drivers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: Since I am using the Nvidia Grid K2, this may not be the most efficient or best way to build docker images for gpu&amp;rsquo;s with cuda capabilities &amp;gt;3.0&lt;/p&gt;

&lt;h3 id=&#34;nvidia-drivers:1d21a3c703f1d61655064657ff3833ef&#34;&gt;Nvidia Drivers:&lt;/h3&gt;

&lt;p&gt;For Docker and Nvidia drivers I have a setup.sh file that uses 352 since 361 seems to cause issues.  A simplified form of what I install (via a setup.sh script) is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-get update &amp;amp;&amp;amp; apt-get install -y sshfs curl wget git htop vim software-properties-common
add-apt-repository ppa:graphics-drivers/ppa -y
apt-get update &amp;amp;&amp;amp; apt-get install -y nvidia-352 nvidia-settings


# Docker and Docker-Compose Stuff
curl -sSL https://get.docker.com/ | sh
curl -L https://github.com/docker/compose/releases/download/1.5.2/docker-compose-`uname -s`-`uname -m` &amp;gt; /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose

git clone https://github.com/NVIDIA/nvidia-docker
cd nvidia-docker &amp;amp;&amp;amp; make install
nvidia-docker volume setup
nvidia-docker run nvidia/cuda nvidia-smi
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;new-dockerfile:1d21a3c703f1d61655064657ff3833ef&#34;&gt;New Dockerfile&lt;/h2&gt;

&lt;p&gt;The new dockerfile isnt terribly different from my original (and would translate easy to just a normal VM on amazon or what not) but requires a few changes from my original file that I am listing for my own clarity.  The full dockerfile is here: &lt;a href=&#34;tensorflow070.Dockerfile&#34;&gt;Dockerfile&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, changing to get-pip.py install of pip3 vs apt-get install python3-pip due to version number and error with protobuf with the severely outdated official 14.04 repo version.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Second, using the latest version of Bazel seems to work which seems smart to track along with due to it being a google product that I am somewhat assuming the tensorflow team uses a recent version of.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Third, setting env so config builds.  Straight forward using the official nvidia dockerfile and locating required files including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CUDA_TOOLKIT_PATH=/usr/local/cuda-7.5&lt;/li&gt;
&lt;li&gt;CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu&lt;/li&gt;
&lt;li&gt;TF_NEED_CUDA=1&lt;/li&gt;
&lt;li&gt;PYTHON_BIN_PATH=/usr/bin/python3&lt;/li&gt;
&lt;li&gt;TF_CUDA_COMPUTE_CAPABILITIES=&amp;ldquo;3.0&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Lastly, symlink the cudnn.h file since it is not in the default CUDNN_INSTALL_PATH:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ln -s /usr/include/cudnn.h /usr/lib/x86_64-linux-gnu/cudnn.h&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these env variables are findable using find | grep (and this works for python 3.5 as well).&lt;/p&gt;

&lt;p&gt;The dockerfile also includes a bunch of extra stuff to install Keras and have it default to TensorFlow (which I then use with a custom script based off the official docker_run_gpu.sh from TensorFlow to customize which GPU is being used by which model being run and allows CPU restriction as well).  While there is other examples of people using TensorFlow with GPU through Docker, all the previous examples were using Python2.7&lt;/p&gt;

&lt;h2 id=&#34;any-questions-or-need-help:1d21a3c703f1d61655064657ff3833ef&#34;&gt;Any questions or need help?&lt;/h2&gt;

&lt;p&gt;I would love to help with any aspects regarding Docker/Keras/TensorFlow
&lt;a href=&#34;mailto:graham.annett@gmail.com&#34;&gt;Email Me&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>using generative neural nets in keras to create â€˜on-the-flyâ€™ dialogue</title>
      <link>http://neuralniche.com/post/tutorial/</link>
      <pubDate>Thu, 10 Sep 2015 12:02:41 -0700</pubDate>
      
      <guid>http://neuralniche.com/post/tutorial/</guid>
      <description>

&lt;h3 id=&#34;introduction:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;Introduction:&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s been a few cool things done with generative neural nets so far but to my knowledge, very few generative neural nets have found a useful application in any publicly discussed business application.  This is by no means the best use or the most interesting, but it is an incredibly interesting idea and is a potential starting point for generative neural nets to be utilized in a way that is incredibly beneficial for training or as an augmentative tool.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s  a lot of potential for this and other similar sorts of technologies and I&amp;rsquo;d love to work on or collaborate with others on something.  If you are interested please contact me at the email listed on the bottom of this post.&lt;/p&gt;

&lt;h2 id=&#34;start:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;Start&lt;/h2&gt;

&lt;p&gt;My initial plan started with the idea that the available subtitles on youtube videos are a good database of conversational dialogue.  This is a massive dataset and could potentially be used to train a lot of different machine learning models.  It may not be &lt;em&gt;perfect&lt;/em&gt;, but it does seem to carry many of the interesting inflections and peculiarities of human speech that written word does not always capture along with having a variety of ways in which the conversations flow.&lt;/p&gt;

&lt;p&gt;The training set to create this model is just a collection of youtube vids that deal with sales or call oriented dialogue.  For instance, here are a couple of videos with subtitles:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=3fbmf2IAEVM&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/3fbmf2IAEVM/0.jpg&#34; alt=&#34;example 1&#34; /&gt;&lt;/a&gt;
&lt;a href=&#34;http://www.youtube.com/watch?v=4ostqJD3Psc&#34;&gt;&lt;img src=&#34;http://img.youtube.com/vi/4ostqJD3Psc/0.jpg&#34; alt=&#34;example 1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There is not a particular reason I used any of these videos other than they are very long, may have phone dialogue, and have subtitles/closed captions already.  I tried to find vids that seemed like the captions were somewhat accurate but there are obvious errors it&amp;rsquo;s effect on the training set is noticeable.&lt;/p&gt;

&lt;p&gt;From here, all that is necessary is to create a fairly large corpus (~500k characters is minimum).&lt;/p&gt;

&lt;p&gt;Using a python script with youtube-dl and pysrt to grab the subtitles/closed captions allows a quick and streamlined pipeline to grab a lot of videos subtitles.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import youtube_dl, pysrt
import numpy as np

class audio_source(object):
    def __init__(self, url):
        self.url = url
        self.ydl_opts = {
            &#39;subtitles&#39;: &#39;en&#39;,
            &#39;writesubtitles&#39;: True,
            &#39;writeautomaticsub&#39;: True}

        self.subtitlesavailable = self.are_subs_available()

        if self.subtitlesavailable:
            self.grab_auto_subs()

    def are_subs_available(self):
        with youtube_dl.YoutubeDL(self.ydl_opts) as ydl:
            subs = ydl.extract_info(self.url, download=False)
        if subs[&#39;requested_subtitles&#39;]:
            self.title = subs[&#39;title&#39;]
            self.subs_url = subs[&#39;requested_subtitles&#39;][&#39;en&#39;][&#39;url&#39;]
            return True
        else:
            return False

    def grab_auto_subs(self):
        &amp;quot;&amp;quot;&amp;quot;
        grab&#39;s subs or cc depending on whats available,
        think it grabs both if subtitles are available
        issue with ydl_opts but doesn&#39;t bother me
        &amp;quot;&amp;quot;&amp;quot;
        try:
            urllib.request.urlretrieve(
                self.subs_url, &#39;youtube-dl-texts/&#39; + self.title + &#39;.srt&#39;)
            print(&amp;quot;subtitles saved directly from youtube\n&amp;quot;)
            text = pysrt.open(&#39;youtube-dl-texts/&#39; + self.title + &#39;.srt&#39;)
            self.text = text.text.replace(&#39;\n&#39;, &#39; &#39;)
        except IOError:
            print(&amp;quot;\n *** saving sub&#39;s didn&#39;t work *** \n&amp;quot;)

with open(&#39;other/url_list&#39;,&#39;r&#39;) as datafile:
    url_list = datafile.read().splitlines()

total_text = []

for u in url_list:
    try:
        total_text.append(audio_source(url=u).text)
    except AttributeError:
        pass
total_text = &#39; &#39;.join(total_text).lower()

print(len(total_text))

&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;training-the-generative-neural-net:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;Training the generative neural net&lt;/h2&gt;

&lt;p&gt;At this point you have a mass of text that if you were to actually read it, would look quite incoherent and useless (also notice I am not creating a separation between texts like many other&amp;rsquo;s have and would probably be very useful in disseminating when a conversation should be ended etc).  There is &lt;em&gt;hopefully&lt;/em&gt; enough data to create an end result for the time being and the errors will &amp;ldquo;regress to the mean&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example of some of the last 260 chars of the dialogue i have from slightly less than 1 MB worth of text from videos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print(total_text[-260:])
&amp;gt;&amp;gt;&amp;gt;&#39;more information about those meetings and travel make sure to fax it to this number at the bottom and are you into the grand prize drawing weeks stay at intercontinental resort Tahiti be sure to fax in that form you all right thank you feel you have a great day&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To train the model we first need to do a bit of preprocessing since the generative neural net uses sequential data character by character (well in &lt;em&gt;steps&lt;/em&gt;, but character by character for each step (a fair amount of this is from the &lt;a href=&#34;https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py&#34;&gt;keras LSTM generating example&lt;/a&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chars = set(total_text)

char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))


maxlen = 20
step = 1
sentences = []
next_chars = []
for i in range(0, len(total_text) - maxlen, step):
    sentences.append(total_text[i: i + maxlen])
    next_chars.append(total_text[i + maxlen])
print(&#39;nb sequences:&#39;, len(sentences))



X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        X[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;lstm-training:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;LSTM training&lt;/h4&gt;

&lt;p&gt;For the NN library, I am using Keras for a few reasons but it is so far my favorite python NN library due to how modular and easy to understand it is (and the &lt;a href=&#34;https://github.com/fchollet/&#34;&gt;creator&lt;/a&gt; and contributors seem incredibly smart).  Quick prototyping and experimentation helps.
For my example, using an LSTM based RNN architecture the most effective way from my experimentation to generate useful results.&lt;/p&gt;

&lt;p&gt;one of the better models i found:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM


model = Sequential()
# input -&amp;gt; layer 1
model.add(LSTM(len(chars), 512, return_sequences=True))
model.add(Dropout(0.20))
# use 20% dropout on all LSTM layers: http://arxiv.org/abs/1312.4569

# 1.5 testing
model.add(LSTM(512, 512, return_sequences=True))
model.add(Dropout(0.20))
# layer 2
model.add(LSTM(512, 256, return_sequences=True))
model.add(Dropout(0.20))
# layer 3
model.add(LSTM(256, 256, return_sequences=False))
model.add(Dropout(0.20))
# layer 4 -&amp;gt; output
model.add(Dense(256, len(chars)))
model.add(Activation(&#39;softmax&#39;))

# compile or load weights then compile depending
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;)

model.fit(X,y,nb_epoch=50)

&amp;gt;&amp;gt;&amp;gt; Epoch 0
&amp;gt;&amp;gt;&amp;gt; 7744/285648 [&amp;gt;.............................] - ETA: 4717s - loss: 3.0232
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;if you would like to see a rudimentary visualization of the architecture:
&lt;img src=&#34;http://neuralniche.com/images/123.png&#34; alt=&#34;nn arch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;this will probably take quite awhile using a personal computer even with a GPU.  From my experimentation it is optimal to get a loss to around .8 - .4 which takes around 40-50 epoch&amp;rsquo;s&lt;/p&gt;

&lt;p&gt;at this point, a model is trained and we are ready to generate some recommended dialogue&lt;/p&gt;

&lt;h2 id=&#34;generate-some-text:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;generate some text&lt;/h2&gt;

&lt;p&gt;the final part of this is being able to speak something to your computer (or potentially, computer would listen to what you or someone else is saying in some app or extension) and from there get the speech into text form to generate a suggestion of what to follow that sentence with.&lt;/p&gt;

&lt;p&gt;there&amp;rsquo;s a few ways to do this but the easiest is to register and get an API key for google speech to text, and install some libraries to be able to use the &lt;a href=&#34;https://pypi.python.org/pypi/SpeechRecognition/&#34;&gt;python speech recognition module&lt;/a&gt;
use a personal key in the Recognizer since otherwise it won&amp;rsquo;t work for other&amp;rsquo;s using the module once it hit&amp;rsquo;s 50 queries.  you need to subscribe to a mailing list and then &lt;a href=&#34;http://www.chromium.org/developers/how-tos/api-keys&#34;&gt;enable the api but it takes about 2 minutes.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;you can incorporate whatever was spoken into the model as well, but that&amp;rsquo;s for a later date.  right now, all i will do is set it up so you speak to it for a moment and then it generates some text and prints that out.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import speech_recognition as sr

recognizer = sr.Recognizer(key=myKey)

def speech2text(r=recognizer):

    # speak to microphone, use google api, return text
    input(&#39;press enter then speak: \n&#39;+&#39;------&#39;*5)
    with sr.Microphone() as source:
        audio = r.listen(source)
        try:
            print(&#39;\nprocessing...\n&#39;)
            return r.recognize(audio).lower()
        except LookupError:
            pass

def gentext():

    seed_text = speech2text()
    generated = &#39;&#39; + seed_text
    print(&#39;------&#39;*5+&#39;\nyou said: \n&#39;+&#39;&amp;quot;&#39; + seed_text +&#39;&amp;quot;&#39;)


    print(&#39;------&#39;*5+&#39;\n generating...\n&#39;+ &#39;------&#39;*5)
    for iteration in range(50):
        # create x vector from seed to predict off of
        x = np.zeros((1, len(seed_text), len(chars)))
        for t, char in enumerate(seed_text):
            x[0, t, char_indices[char]] = 1.

        preds = model.predict(x, verbose=0)[0]
        next_index = np.argmax(preds)
        next_char = indices_char[next_index]

        generated += next_char
        seed_text = seed_text[1:] + next_char
    print(&#39;\n\nfollow up with: &#39; + generated)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;here&amp;rsquo;s one of the best single example&amp;rsquo;s i&amp;rsquo;ve found with this model and have gotten a lot different results from a variety of attempts and architecture&amp;rsquo;s and planning on posting more if it would be useful (or can post the exact architecture and model weights which keras can then load):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gentext()

press enter then speak:
------------------------------

processing...

------------------------------
you said:
&amp;quot;i would like to talk to you about a house i saw that you had for sale&amp;quot;
------------------------------
 generating...
------------------------------

follow up with:
i would like to talk to you about a house i saw that you had for sale tell me what was its price though and i can reall
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;note:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;note:&lt;/h4&gt;

&lt;p&gt;in terms of training the model, training/predicting with a GPU vs CPU is about 3-4x faster on my 2013 macbook pro&lt;/p&gt;

&lt;h2 id=&#34;conclusion:94d9a081cd1256334373c8ca6fb6276c&#34;&gt;conclusion&lt;/h2&gt;

&lt;p&gt;with something like this, it&amp;rsquo;s very easy to see how you could splice in audio from a phone call or text chat that this would carry over very well to.  given the right data set&amp;rsquo;s theres tons of potential uses.  along with this, there&amp;rsquo;s also ways to stack and blend models together that provide different and separate different dialogue/differentiate people within dialogue.&lt;/p&gt;

&lt;p&gt;this all came about because after the yc fellowship rejection, i thought i may as well post this since i personally think it has a ton of potential.  if you are interested in hearing more about this, or hearing more about this type of stuff, contact me at the email posted below or signup for a newsletter.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;a href=&#34;mailto:graham.annett@gmail.com&#34;&gt;contact me&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;link href=&#34;//cdn-images.mailchimp.com/embedcode/slim-081711.css&#34; rel=&#34;stylesheet&#34; type=&#34;text/css&#34;&gt;
&lt;style type=&#34;text/css&#34;&gt;
    #mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }
    /* Add your own MailChimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
&lt;/style&gt;
&lt;div id=&#34;mc_embed_signup&#34;&gt;
&lt;form action=&#34;//neuralniche.us11.list-manage.com/subscribe/post?u=7f3e6432894032f97ce9da591&amp;amp;id=a86a7392be&#34; method=&#34;post&#34; id=&#34;mc-embedded-subscribe-form&#34; name=&#34;mc-embedded-subscribe-form&#34; class=&#34;validate&#34; target=&#34;_blank&#34; novalidate&gt;
&lt;div id=&#34;mc_embed_signup_scroll&#34;&gt;
&lt;label for=&#34;mce-EMAIL&#34;&gt;Subscribe to our mailing list&lt;/label&gt;
&lt;input type=&#34;email&#34; value=&#34;&#34; name=&#34;EMAIL&#34; class=&#34;email&#34; id=&#34;mce-EMAIL&#34; placeholder=&#34;email address&#34; required&gt;
&lt;div style=&#34;position: absolute; left: -5000px;&#34;&gt;&lt;input type=&#34;text&#34; name=&#34;b_7f3e6432894032f97ce9da591_a86a7392be&#34; tabindex=&#34;-1&#34; value=&#34;&#34;&gt;&lt;/div&gt;
&lt;div class=&#34;clear&#34;&gt;&lt;input type=&#34;submit&#34; value=&#34;Subscribe&#34; name=&#34;subscribe&#34; id=&#34;mc-embedded-subscribe&#34; class=&#34;button&#34;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/form&gt;
&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>hey!</title>
      <link>http://neuralniche.com/post/first/</link>
      <pubDate>Wed, 19 Aug 2015 12:02:41 -0700</pubDate>
      
      <guid>http://neuralniche.com/post/first/</guid>
      <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Will update&lt;/p&gt;

&lt;div class=&#34;sendgrid-subscription-widget&#34; data-token=&#34;s%2FyRM37csnDZE2YNQgMgA%2FrRJMnvJB6PyYYUOnu3QGxoLuVzko05dg04VrpUhqXh&#34;
    &lt;form&gt;
        &lt;div class=&#34;response&#34;&gt;&lt;/div&gt;
        &lt;label&gt;
        &lt;br /&gt;
            &lt;span&gt;interested in using this, enter email below!&lt;/span&gt;
            &lt;br /&gt;
            &lt;br /&gt;
            &lt;input type=&#34;email&#34; name=&#34;email&#34; placeholder=&#34;enter email&#34; /&gt;
        &lt;/label&gt;
        &lt;input type=&#34;submit&#34; value=&#34;submit :)&#34; /&gt;
    &lt;/form&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>